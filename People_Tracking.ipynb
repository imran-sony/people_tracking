{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy2J-U3RW7M8"
      },
      "source": [
        "# People Flow Detection using Object Tracking & Heatmap Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GHgmcW_wXCO0",
        "outputId": "0f88e9ed-9fc9-49a2-8bd6-491c935ba698"
      },
      "outputs": [],
      "source": [
        "!pip install supervision ultralytics roboflow pytesseract rfdetr inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiXy6nvgO8GD",
        "outputId": "4aa15dc8-fa83-44a4-b822-e1017b871963"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import sys\n",
        "from IPython.display import Video\n",
        "\n",
        "# Settings\n",
        "SLOWDOWN_MS = 50  # not needed for Colab but kept for consistency\n",
        "VIDEO_PATH = 'https://media.roboflow.com/supervision/video-examples/people-walking.mp4'\n",
        "MODEL_PATH = 'yolo11x.pt'     # YOLO model\n",
        "#MODEL_PATH = \"yolov8n.pt\"  # YOLOv8 small model\n",
        "OUTPUT_PATH = \"people_count_output.mp4\"\n",
        "\n",
        "# Annotation coordinates\n",
        "ANNOT_LINE_IN_START = (5, 380)\n",
        "ANNOT_LINE_IN_END = (1915, 380)\n",
        "ANNOT_LINE_OUT_START = (5, 720)\n",
        "ANNOT_LINE_OUT_END = (1915, 720)\n",
        "ANNOT_W, ANNOT_H = 1920, 1080\n",
        "\n",
        "\n",
        "# Person class ID in COCO dataset\n",
        "PERSON_CLASS_ID = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpZCGmXLPBcq",
        "outputId": "a89cd897-73d5-4d79-f7d7-71a6ae94819c"
      },
      "outputs": [],
      "source": [
        "def process_video(video_path=VIDEO_PATH, output_path=OUTPUT_PATH):\n",
        "    try:\n",
        "        model = YOLO(MODEL_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Failed to load YOLO model from '{MODEL_PATH}'.\")\n",
        "        print(\"Error details:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Failed to open video file.\")\n",
        "        return\n",
        "\n",
        "    # Read first frame to get size\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read first frame.\")\n",
        "        return\n",
        "    orig_h, orig_w = frame.shape[:2]\n",
        "\n",
        "    # Scale annotation coordinates\n",
        "    def scale_point(pt):\n",
        "        x, y = pt\n",
        "        return (int(x * orig_w / ANNOT_W), int(y * orig_h / ANNOT_H))\n",
        "\n",
        "    LINE_IN_START = scale_point(ANNOT_LINE_IN_START)\n",
        "    LINE_IN_END = scale_point(ANNOT_LINE_IN_END)\n",
        "    LINE_OUT_START = scale_point(ANNOT_LINE_OUT_START)\n",
        "    LINE_OUT_END = scale_point(ANNOT_LINE_OUT_END)\n",
        "\n",
        "    # Prepare output writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20, (960, 540))\n",
        "\n",
        "    # Reset video to start\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    in_count, out_count = 0, 0\n",
        "    already_counted_in, already_counted_out = set(), set()\n",
        "    track_history = {}\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        results = model.track(frame, persist=True, tracker=\"botsort.yaml\", verbose=False)[0]\n",
        "        boxes = results.boxes\n",
        "        if boxes is None or boxes.xyxy is None:\n",
        "            display_frame = cv2.resize(frame, (960, 540))\n",
        "            out.write(display_frame)\n",
        "            continue\n",
        "\n",
        "        class_ids = boxes.cls.cpu().numpy().astype(int)\n",
        "        xyxy = boxes.xyxy.cpu().numpy()\n",
        "        track_ids = boxes.id.cpu().numpy().astype(int) if boxes.id is not None else [None]*len(xyxy)\n",
        "        person_indices = [i for i, cid in enumerate(class_ids) if cid == PERSON_CLASS_ID]\n",
        "\n",
        "        # Draw lines\n",
        "        cv2.line(frame, LINE_IN_START, LINE_IN_END, (255, 0, 0), 2)\n",
        "        cv2.line(frame, LINE_OUT_START, LINE_OUT_END, (0, 0, 255), 2)\n",
        "\n",
        "        for idx in person_indices:\n",
        "            x1, y1, x2, y2 = map(int, xyxy[idx])\n",
        "            track_id = track_ids[idx]\n",
        "            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
        "\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append(center)\n",
        "            if len(track_history[track_id]) > 2:\n",
        "                track_history[track_id] = track_history[track_id][-2:]\n",
        "\n",
        "            if len(track_history[track_id]) == 2:\n",
        "                prev_center, curr_center = track_history[track_id]\n",
        "\n",
        "                # Line IN\n",
        "                if (LINE_IN_START[0] <= prev_center[0] <= LINE_IN_END[0] and\n",
        "                    LINE_IN_START[0] <= curr_center[0] <= LINE_IN_END[0]):\n",
        "                    if prev_center[1] < LINE_IN_START[1] <= curr_center[1] and track_id not in already_counted_in:\n",
        "                        in_count += 1\n",
        "                        already_counted_in.add(track_id)\n",
        "\n",
        "                # Line OUT\n",
        "                if (LINE_OUT_START[0] <= prev_center[0] <= LINE_OUT_END[0] and\n",
        "                    LINE_OUT_START[0] <= curr_center[0] <= LINE_OUT_END[0]):\n",
        "                    if prev_center[1] > LINE_OUT_START[1] >= curr_center[1] and track_id not in already_counted_out:\n",
        "                        out_count += 1\n",
        "                        already_counted_out.add(track_id)\n",
        "\n",
        "            # Draw bbox + ID\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
        "            cv2.circle(frame, center, 5, (0, 255, 0), -1)\n",
        "            cv2.putText(frame, f'ID {track_id}', (center[0] - 10, center[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "\n",
        "        # Counts\n",
        "        cv2.putText(frame, f'In: {in_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2.2, (0,255,0), 5)\n",
        "        cv2.putText(frame, f'Out: {out_count}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 2.2, (0,0,255), 5)\n",
        "\n",
        "        display_frame = cv2.resize(frame, (960, 540))\n",
        "        out.write(display_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"‚úÖ Processing complete. Saved to {output_path}\")\n",
        "\n",
        "# Run\n",
        "process_video(VIDEO_PATH, \"people_count_output.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CDSS7wHPCad",
        "outputId": "8118f5de-7aa0-4716-abbf-578248a06bbf"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "# Settings\n",
        "OUTPUT_PATH = 'heatmap_output.mp4'\n",
        "\n",
        "def process_heatmap(video_path=VIDEO_PATH, output_path=OUTPUT_PATH):\n",
        "    # Load YOLO model\n",
        "    model = YOLO(MODEL_PATH)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"‚ùå Failed to open video file.\")\n",
        "        return\n",
        "\n",
        "    # Read first frame to get frame size\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"‚ùå Failed to read first frame.\")\n",
        "        return\n",
        "\n",
        "    orig_h, orig_w = frame.shape[:2]\n",
        "\n",
        "    # Initialize Supervision Heatmap Annotator\n",
        "    heatmap_annotator = sv.HeatMapAnnotator(\n",
        "        opacity=0.6,\n",
        "        radius=40,\n",
        "        kernel_size=25,\n",
        "        top_hue=0,       # red (high density)\n",
        "        low_hue=120      # blue (low density)\n",
        "    )\n",
        "\n",
        "    # Prepare output writer\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 20, (orig_w, orig_h))\n",
        "\n",
        "    # Reset video to start\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    frame_count = 0\n",
        "\n",
        "    print(\"üöÄ Generating heatmap...\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detect people\n",
        "        results = model(frame, verbose=False)[0]\n",
        "        detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "        # Keep only 'person' class (ID = 0 in COCO)\n",
        "        mask = detections.class_id == 0  # person only\n",
        "        detections = detections[mask]\n",
        "\n",
        "        # Annotate with heatmap\n",
        "        frame_with_heatmap = heatmap_annotator.annotate(scene=frame.copy(), detections=detections)\n",
        "\n",
        "        out.write(frame_with_heatmap)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress logging\n",
        "        if frame_count % 50 == 0:\n",
        "            print(f\"Processed {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"‚úÖ Heatmap video saved to {output_path}\")\n",
        "\n",
        "# Run\n",
        "process_heatmap()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
